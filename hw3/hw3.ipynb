{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li0bVCTuxc6n"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "#### Lab 3\n",
    "\n",
    "# National Tsing Hua University\n",
    "\n",
    "#### Spring 2025\n",
    "\n",
    "#### 11320IEEM 513600\n",
    "\n",
    "#### Deep Learning and Industrial Applications\n",
    "    \n",
    "## Lab 3: Anomaly Detection in Industrial Applications\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlvflhYwCu8Q"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In today's industrial landscape, the ability to detect anomalies in manufacturing processes and products is critical for maintaining quality, efficiency, and safety. This lab focuses on leveraging deep learning techniques for anomaly detection in various industrial applications, using the MVTEC Anomaly Detection Dataset. By employing ImageNet-pretrained models available in torchvision, students will gain hands-on experience in classfying defects and irregularities across different types of industrial products.\n",
    "\n",
    "Throughout this lab, you'll be involved in the following key activities:\n",
    "- Explore and process the MVTec Anomaly Detection Dataset.\n",
    "- Apply ImageNet-pretrained models from [Torchvision](https://pytorch.org/vision/stable/models.html) to detect anomalies in industrial products.\n",
    "- Evaluate the performance of the models to understand their effectiveness in real-world industrial applications.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand the principles of anomaly detection in the context of industrial applications.\n",
    "- Learn how to implement and utilize ImageNet-pretrained models for detecting anomalies.\n",
    "- Analyze and interpret the results of the anomaly detection models to assess their practicality in industrial settings.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The MVTec AD Dataset is a comprehensive collection of high-resolution images across different categories of industrial products, such as bottles, cables, and metal nuts, each with various types of defects. This dataset is pivotal for developing and benchmarking anomaly detection algorithms. You can download our lab's dataset [here](https://drive.google.com/file/d/19600hUOpx0hl78TdpdH0oyy-gGTk_F_o/view?usp=share_link). You can drop downloaded data and drop to colab, or you can put into yor google drive.\n",
    "\n",
    "### References\n",
    "- [MVTec AD Dataset](https://www.kaggle.com/datasets/ipythonx/mvtec-ad/data) for the dataset used in this lab.\n",
    "- [Torchvision Models](https://pytorch.org/vision/stable/models.html) for accessing ImageNet-pretrained models to be used in anomaly detection tasks.\n",
    "- [State-of-the-Art Anomaly Detection on MVTec AD](https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad) for insights into the latest benchmarks and methodologies in anomaly detection applied to the MVTec AD dataset.\n",
    "- [CVPR 2019: MVTec AD â€” A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection] for the original paper of MVTec AD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuiEw1L0Cu8Q"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uA8NjVtWga8w",
    "outputId": "c646601d-bf56-4f1f-b127-302b5f7bb9bd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXfjTWKUCu8R"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/lab3')\n",
    "file_paths = glob.glob(r'Dataset/cable/*/*/*.png')\n",
    "# file_paths = sorted([path for path in file_paths if path.split('\\\\')[-1] in [f'{i:03}.png' for i in range(10)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "842a02260c024cb5a72520ece69b70c0",
      "dfb7c23c15d44a13900df82022a3045f",
      "900c595b15214e34a8fb3ca916d414b4",
      "2b4e5df2b9ba4f0185946790cd196f75",
      "d6b580e2140a4db39b88c5d0d8deb15d",
      "1d035b2f5d2b4bff89ded47b9fea82e1",
      "6affd735e56242d0a2467321f6da1d5b",
      "de31bb2e792a4eef9cbee1d24a8fb441",
      "7836fe918ac640949f581f6be498de57",
      "1c71dc94a1074e5cb51f1d7909f389cd",
      "b5a6578f00924e21b3e04fa9a7b31096"
     ]
    },
    "id": "3GiOZBRJCu8S",
    "outputId": "6c72a90a-77cc-4ea2-9e8f-7953b4972ce1"
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "for img in tqdm(file_paths):\n",
    "    img = cv2.imread(img)\n",
    "    img = img[..., ::-1]\n",
    "    all_data.append(img)\n",
    "\n",
    "all_data = np.stack(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ii8LH8s4Cu8S",
    "outputId": "5508ef1f-2bc1-4dbb-b63a-4b360a399df7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "classes = sorted(set([path.split('/')[2] for path in file_paths]))\n",
    "print(f'Classes: {classes}')\n",
    "\n",
    "path = 'Dataset/cable/'\n",
    "\n",
    "# for cls in classes:\n",
    "#     count = 0\n",
    "#     for path in file_paths:\n",
    "#         if path.split('\\\\')[1] == cls:\n",
    "#             count += 1\n",
    "#     print(f\"{cls}: {count}\")\n",
    "for cls in classes:\n",
    "    sub_path = os.path.join(path, cls)\n",
    "\n",
    "    n = len(glob.glob(f\"{sub_path}/*/*.png\"))\n",
    "    print(f'{cls}:{n}, {n - len(glob.glob(f\"{sub_path}/good/*.png\"))}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-1PsC--M7pT"
   },
   "source": [
    "## A. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "8079712851784d53bb477608193f0cae",
      "160ce66730d541f7b4f2631626529344",
      "e533c1b533c34720a967b591a265ec04",
      "60768bfbef4642a8a6c220aaa6e5aa49",
      "80a2e161054140e9a7be5d896544c4df",
      "94b2c9d65c024724a99e14648d885b96",
      "e8f0333ea9584f4ab4bbc8ef1b7de889",
      "b93bcd38bfb6434b9d8870163a33b3b2",
      "9ac960f9f3834f36b9de3863c35e3ae8",
      "62968d516bc14d6195f8fe9a6abdb087",
      "81241cf1c1214b7c80a299334708b23e"
     ]
    },
    "id": "nGFI8GMpCu8S",
    "outputId": "b027d7c1-c28d-446d-9b18-75e65c60a0ae"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "\n",
    "all_data = []\n",
    "file_paths = glob.glob(r'Dataset/cable/train/*/*.png')\n",
    "\n",
    "for img in tqdm(file_paths):\n",
    "    img = cv2.imread(img)\n",
    "    img = img[..., ::-1]\n",
    "    all_data.append(img)\n",
    "\n",
    "all_data = np.stack(all_data)\n",
    "assert all_data.shape[0] == 224\n",
    "\n",
    "num_classes = 2\n",
    "train_n = 224\n",
    "\n",
    "train_n = int(train_n * 0.8)\n",
    "val_n = 224 - train_n\n",
    "\n",
    "x_train = all_data[:train_n]\n",
    "x_val = all_data[train_n:]\n",
    "\n",
    "\n",
    "# The shape changes from (batch_size, height, width, channels) to (batch_size, channels, height, width)\n",
    "x_train = np.transpose(np.array(x_train), (0, 3, 1, 2))\n",
    "x_val = np.transpose(np.array(x_val), (0, 3, 1, 2))\n",
    "\n",
    "y_train = np.ones(x_train.shape[0])\n",
    "y_val = np.ones(x_val.shape[0])\n",
    "\n",
    "print(f'Shape of x_train: {x_train.shape}')\n",
    "print(f'Shape of x_val: {x_val.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of y_val: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-CnfsmbCu8T"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.AutoAugment(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        new_x = np.transpose(self.x[idx], (1, 2, 0))\n",
    "        return self.transform(Image.fromarray(new_x)), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53ZVFFacCu8T"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataset = MyDataset(x_train, y_train, train_transforms)\n",
    "val_dataset = MyDataset(x_val, y_val, val_transforms)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaLGtT28xc6s"
   },
   "source": [
    "## B. Defining Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDX8iDKJCu8U",
    "outputId": "b27ad10d-6779-4ce9-826b-7f67dba35a02"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# ConvNet as fixed feature extractor (freeze parameters)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "num_class = 2\n",
    "\n",
    "# change # of class from 1000 into 8 in the last layer\n",
    "model.fc = nn.Linear(num_ftrs, num_class)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvLTU-IfZLqn"
   },
   "source": [
    "## C. Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917,
     "referenced_widgets": [
      "e76887b8cbb34e7795237c878f70c50a",
      "96c1c2ea5e1d460bbf81761467ce460d",
      "15f9482fb0b54b288b7277672bc9c28b",
      "eaa4b842a79b4a3ebd4cc75e471199ab",
      "8a6615136dae40b9965ea68551b717cf",
      "8ca1823bc2624986a5fba4ae2a064939",
      "df4e6c8d8aea468cb648a3be42649b4a",
      "dee06cfa642f4d9cb11140df89c16833",
      "68d27ea474254b5293f2a0df09673c01",
      "52e89e09587647879a074262efa94268",
      "280b858572904f30b3d39131a78a1129"
     ]
    },
    "id": "45ol4lpVxc6t",
    "outputId": "7697a413-8710-4219-850c-790cead4a07a"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "epochs = 50\n",
    "model = model.to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = -1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader)*epochs, eta_min=0)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        images = (images) / 255.\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        labels = labels.long()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        train_predicted = outputs.argmax(-1)\n",
    "        train_correct += (train_predicted == labels).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100. * train_correct / total_train_samples\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            images = (images) / 255.\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            labels = labels.long()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            predicted = outputs.argmax(-1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = 100. * correct / total\n",
    "\n",
    "    # Learning rate update\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Checkpoint\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'model_classification.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.4f}%, Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.4f}%, Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
    "\n",
    "    # Store performance\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjmYxAJnxc6t"
   },
   "source": [
    "### Visualizing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "pHpS0Q7vxc6t",
    "outputId": "e62a3826-d730-4332-af77-4e2b4f97531f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "ax[0].plot(train_accuracies)\n",
    "ax[0].plot(val_accuracies)\n",
    "ax[0].set_title('Model Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['Train', 'Val'])\n",
    "\n",
    "# Plotting training and validation loss\n",
    "ax[1].plot(train_losses)\n",
    "ax[1].plot(val_losses)\n",
    "ax[1].set_title('Model Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['Train', 'Val'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVDWBwv6Cu8V"
   },
   "source": [
    "## D. Evaluating Your Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEztHBDjCu8V"
   },
   "source": [
    "### Load Trained Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bfc36a97f8d54d7d963cb22b475e3237",
      "5181d4b4c35949c7905245e9286d02da",
      "21df7426ee4d4752bb35328a85dde8e5",
      "e9ab3f62c29d46fcab28390245ad66b6",
      "2744b8d0b2974a408a9f8c19e95d7558",
      "33aacdc8bce44be294d6b433e2aae45e",
      "07a45357c7d8413f9e5b3470690f8250",
      "f8518910303b4efab82a2fb4992cba5c",
      "52fbfeadfd05465bb1497a47c6d271b3",
      "6062061d80de487eaa5dd7fc585a5adf",
      "9d14f675ea8848d0b473a6bc43906fce"
     ]
    },
    "id": "2DA1qHXpCu8V",
    "outputId": "01b4911a-5bcc-4c4d-f6ce-97e5241911a0"
   },
   "outputs": [],
   "source": [
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('model_classification.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "file_paths = glob.glob(r'Dataset/cable/test/*/*.png')\n",
    "\n",
    "for img in tqdm(file_paths):\n",
    "    if img.split('/')[3] == 'good':\n",
    "      y_test.append(1)\n",
    "    else:\n",
    "      y_test.append(0)\n",
    "    img = cv2.imread(img)\n",
    "    img = img[..., ::-1]\n",
    "    x_test.append(img)\n",
    "\n",
    "x_test = np.stack(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "x_test = np.transpose(np.array(x_test), (0, 3, 1, 2))\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "test_dataset = MyDataset(x_test, y_test, val_transforms)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        images = images.cuda()\n",
    "        images = (images) / 255.\n",
    "\n",
    "        labels = labels.cuda()\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        predicted = outputs.argmax(-1)\n",
    "        # print(predicted)\n",
    "        # print(labels)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f'Test accuracy is {100. * test_correct / test_total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4iA5GoprEb5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGjqlK6OvH74"
   },
   "source": [
    "# Method1 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IU2hCuMvoP4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ResNetAutoencoder(nn.Module):\n",
    "    def __init__(self, encoded_dim=512):\n",
    "        super(ResNetAutoencoder, self).__init__()\n",
    "\n",
    "        # Use pretrained ResNet18 as encoder\n",
    "        resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])  # remove fc layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder - you can customize this\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoded_dim, 8*8*64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (64, 8, 8)),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
    "            nn.Sigmoid()  # to keep pixel range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.flatten(z)\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "model = ResNetAutoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917,
     "referenced_widgets": [
      "2f967cbbbd5b4af79a37ee1a914c2990",
      "27643860bb4145809c601615163c8a92",
      "a3711e52266d494492d5eaea725bfb5a",
      "637390b2890f4ce1b98c01ab028acdb2",
      "982b7e2987f2495ba772886929ac6f53",
      "32eabf4d533d493ab272f8a7a2078238",
      "42c887032ff44922a1b86701664cf417",
      "a2ce4a40030f45f7959b735cb825e502",
      "bb2438c40b84409aac99aad0a8231947",
      "b008f08253af443ca70ba3aa4344fc22",
      "fa42d4d6f8174620a25260795398c679"
     ]
    },
    "id": "SGRy4CHlvr8F",
    "outputId": "cd15483c-e941-41e0-c233-570d183d8750"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "epochs = 50\n",
    "model = model.to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = -1\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader)*epochs, eta_min=0)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train_samples = 0\n",
    "    all_reconstruction_errors = []\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        images = images.to(device)\n",
    "        images = (images) / 255.\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        labels = labels.long()\n",
    "\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        error = torch.mean((outputs - images) ** 2, dim=[1, 2, 3])  # per image\n",
    "        all_reconstruction_errors.extend(error.detach().cpu().numpy())\n",
    "        threshold = np.percentile(all_reconstruction_errors, 95)  # e.g., top 5% as anomaly\n",
    "\n",
    "        # Predict\n",
    "        train_predicted = np.array([1 if err > threshold else 0 for err in error.detach().cpu().numpy()])\n",
    "        labels_cpu = labels.detach().cpu().numpy()\n",
    "        train_correct += (train_predicted == labels_cpu).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100. * train_correct / total_train_samples\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            images = (images) / 255.\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            error = torch.mean((outputs - images) ** 2, dim=[1, 2, 3])  # per image\n",
    "\n",
    "            predicted = np.array([1 if err > threshold else 0 for err in error.detach().cpu().numpy()])\n",
    "            labels = labels.long()\n",
    "\n",
    "            loss = criterion(outputs, images)\n",
    "            total_val_loss += loss.item()\n",
    "            labels_cpu = labels.detach().cpu().numpy()\n",
    "            correct += (predicted == labels_cpu).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = 100. * correct / total\n",
    "\n",
    "    # Learning rate update\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Checkpoint\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'model_classification.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.4f}%, Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.4f}%, Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
    "\n",
    "    # Store performance\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "f8d8a0a99d1b481bbb228dfb68e7eda7",
      "a9dd9c7dd03c42dbadc4693a465a856b",
      "eee25460cd9341a9b07dae482cb8bcad",
      "5f349241343549f5921310a2fcc72997",
      "dcf3758f89474f53ad961b34423376f6",
      "80bb0563dc8e4744ac173693fe42bf24",
      "e795349bbb2e474d857c5603e1cb2064",
      "6d0a40b5afcf423995996a6fb9a34b74",
      "619ca5a2a30d485196cb73368107a2d7",
      "6faf9840bfbc4fd8a7b6a55888603fae",
      "089242314cc94a799b7198cefd7c31ad"
     ]
    },
    "id": "df5YHLTRx_6x",
    "outputId": "e85c653d-9377-4392-e82b-ae9cc1feb5d4"
   },
   "outputs": [],
   "source": [
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('model_classification.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "file_paths = glob.glob(r'Dataset/cable/test/*/*.png')\n",
    "\n",
    "for img in tqdm(file_paths):\n",
    "    if img.split('/')[3] == 'good':\n",
    "      y_test.append(1)\n",
    "    else:\n",
    "      y_test.append(0)\n",
    "    img = cv2.imread(img)\n",
    "    img = img[..., ::-1]\n",
    "    x_test.append(img)\n",
    "\n",
    "x_test = np.stack(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "x_test = np.transpose(np.array(x_test), (0, 3, 1, 2))\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "test_dataset = MyDataset(x_test, y_test, val_transforms)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        reconstruction_errors = []\n",
    "        images = images.to(device)\n",
    "        images = (images) / 255.\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        error = torch.mean((outputs - images) ** 2, dim=[1, 2, 3])  # per image\n",
    "        #reconstruction_errors.extend(error.detach().cpu().numpy())\n",
    "        #threshold = np.percentile(reconstruction_errors, 95)  # e.g., top 5% as anomaly\n",
    "\n",
    "        predicted = np.array([1 if err > threshold else 0 for err in error.detach().cpu().numpy()])\n",
    "        labels = labels.long()\n",
    "\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        labels_cpu = labels.detach().cpu().numpy()\n",
    "        test_correct += (predicted == labels_cpu).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f'Test accuracy is {100. * test_correct / test_total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhIpyt4z8dEN"
   },
   "source": [
    "# Method2 Decoder with Deeper Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_omDkosN0ejh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ResNetAutoencoder(nn.Module):\n",
    "    def __init__(self, encoded_dim=512):\n",
    "        super(ResNetAutoencoder, self).__init__()\n",
    "\n",
    "        # Use pretrained ResNet50 as encoder\n",
    "        resnet = models.resnet50(weights='IMAGENET1K_V1')\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])  # remove fc layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder - you can customize this\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoded_dim, 8*8*64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (64, 8, 8)),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
    "            nn.Sigmoid()  # to keep pixel range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.flatten(z)\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "model = ResNetAutoencoder(encoded_dim=2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917,
     "referenced_widgets": [
      "802ee82ce71e46c2827b95839e50b3b7",
      "d7f8830a95be410694c45284d02cc4e2",
      "8f87ca1b6a704664b3566ba5fcbedaef",
      "b59df312531c478ab973e4fea322eafc",
      "4d4d8ec09138485b90a42b8f915bfa7e",
      "512a20d5f9504719ad121c6e40d8244c",
      "c9dbc9419aed4168bd7eed80f37fe948",
      "a62b75a301ce4856bb208b373cc62c5c",
      "2fda2a78be67441f93fd052951d88ade",
      "a97816890cd649a7803ff37151ad7c24",
      "ff88a8023bbf49efa6acb8590dc93388"
     ]
    },
    "id": "7uFzmacf8sCE",
    "outputId": "bc55c24d-6c2a-4f91-d4e0-7b42ec59d2cf"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "epochs = 50\n",
    "model = model.to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = -1\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader)*epochs, eta_min=0)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train_samples = 0\n",
    "    all_reconstruction_errors = []\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        images = images.to(device)\n",
    "        images = (images) / 255.\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        labels = labels.long()\n",
    "\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        error = torch.mean((outputs - images) ** 2, dim=[1, 2, 3])  # per image\n",
    "        all_reconstruction_errors.extend(error.detach().cpu().numpy())\n",
    "        threshold = np.percentile(all_reconstruction_errors, 95)  # e.g., top 5% as anomaly\n",
    "\n",
    "        # Predict\n",
    "        train_predicted = np.array([1 if err > threshold else 0 for err in error.detach().cpu().numpy()])\n",
    "        labels_cpu = labels.detach().cpu().numpy()\n",
    "        train_correct += (train_predicted == labels_cpu).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100. * train_correct / total_train_samples\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            images = (images) / 255.\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            error = torch.mean((outputs - images) ** 2, dim=[1, 2, 3])  # per image\n",
    "\n",
    "            predicted = np.array([1 if err > threshold else 0 for err in error.detach().cpu().numpy()])\n",
    "            labels = labels.long()\n",
    "\n",
    "            loss = criterion(outputs, images)\n",
    "            total_val_loss += loss.item()\n",
    "            labels_cpu = labels.detach().cpu().numpy()\n",
    "            correct += (predicted == labels_cpu).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = 100. * correct / total\n",
    "\n",
    "    # Learning rate update\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Checkpoint\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'model_classification.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.4f}%, Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.4f}%, Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
    "\n",
    "    # Store performance\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "ae8edf788c20484092472136fcec1668",
      "381c88ea695a4243bf7f2c047438df5d",
      "2a08341766804e50a6628f7bb89e7e9f",
      "0decb3b023934afdb5a6bb8432024204",
      "19bad384293a4255b1f1c495ff5732b0",
      "fcee278ec5e5498b825ae7fa49540088",
      "4156e743c7034b988da493b254f078e4",
      "9d219b123ba746ef94f5cb906e505410",
      "ee886e678e2e4523913af754086c21bd",
      "ff7864d515234895b4fb7db4a1d1b1c8",
      "7f3e4ac3856a41b28b74b490406608ce"
     ]
    },
    "id": "Vc2Wm5dy8vAX",
    "outputId": "e8e76149-b259-4bc7-aa07-4f9f2b0d3008"
   },
   "outputs": [],
   "source": [
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('model_classification.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "file_paths = glob.glob(r'Dataset/cable/test/*/*.png')\n",
    "\n",
    "for img in tqdm(file_paths):\n",
    "    if img.split('/')[3] == 'good':\n",
    "      y_test.append(1)\n",
    "    else:\n",
    "      y_test.append(0)\n",
    "    img = cv2.imread(img)\n",
    "    img = img[..., ::-1]\n",
    "    x_test.append(img)\n",
    "\n",
    "x_test = np.stack(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "x_test = np.transpose(np.array(x_test), (0, 3, 1, 2))\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "test_dataset = MyDataset(x_test, y_test, val_transforms)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        reconstruction_errors = []\n",
    "        images = images.to(device)\n",
    "        images = (images) / 255.\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        error = torch.mean((outputs - images) ** 2, dim=[1, 2, 3])  # per image\n",
    "        #reconstruction_errors.extend(error.detach().cpu().numpy())\n",
    "        #threshold = np.percentile(reconstruction_errors, 95)  # e.g., top 5% as anomaly\n",
    "\n",
    "        predicted = np.array([1 if err > threshold else 0 for err in error.detach().cpu().numpy()])\n",
    "        labels = labels.long()\n",
    "\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        labels_cpu = labels.detach().cpu().numpy()\n",
    "        test_correct += (predicted == labels_cpu).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f'Test accuracy is {100. * test_correct / test_total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYTVBdSc-flJ"
   },
   "source": [
    "# Method3 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlFUSbp1_QIZ",
    "outputId": "e14ecdf2-ec3f-41a3-da02-595954857743"
   },
   "outputs": [],
   "source": [
    "pip install pytorch-msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "6481eeccaefc46d3afcbc177e77c42db",
      "d1c46d85f5b4463c8030dcebb96e4e40",
      "3a050cc00be04559a42ec33266d1689c",
      "8ba9d8cdc928440093e5d64aa923fec5",
      "64baac50a9164604a846144c48ae02bf",
      "ef56041a6e474cfaa0778be6c3734b5d",
      "507533fd83b2491a9e2d8070c885167a",
      "f6ab0b1a46eb4140bf453b957d62e916",
      "cc56c0dc39cb490aaece99ba0194ba52",
      "ccfc8275ab4048389210aa97fe2c4b69",
      "8e3dbfbc98994531bff63dd84a01803c"
     ]
    },
    "id": "psyejzG09Iqr",
    "outputId": "2e8d3606-fa95-4a37-d1e2-b3712dc34ba2"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "def ssim_loss(pred, target):\n",
    "    return 1 - ssim(pred, target, data_range=1.0, size_average=True)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "model = ResNetAutoencoder(encoded_dim=2048).to(device)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "epochs = 5\n",
    "model = model.to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = -1\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "def combined_loss(pred, target):\n",
    "    return 0.5 * mse(pred, target) + 0.5 * ssim_loss(pred, target)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader)*epochs, eta_min=0)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train_samples = 0\n",
    "    all_reconstruction_errors = []\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        images = images.to(device)\n",
    "        images = (images) / 255.\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        labels = labels.long()\n",
    "\n",
    "        loss = combined_loss(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        error = torch.mean((outputs - images) ** 2, dim=[1, 2, 3])  # per image\n",
    "        all_reconstruction_errors.extend(error.detach().cpu().numpy())\n",
    "        threshold = np.percentile(all_reconstruction_errors, 95)  # e.g., top 5% as anomaly\n",
    "\n",
    "        # Predict\n",
    "        train_predicted = np.array([1 if err > threshold else 0 for err in error.detach().cpu().numpy()])\n",
    "        labels_cpu = labels.detach().cpu().numpy()\n",
    "        train_correct += (train_predicted == labels_cpu).sum().item()\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100. * train_correct / total_train_samples\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            images = (images) / 255.\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            error = torch.mean((outputs - images) ** 2, dim=[1, 2, 3])  # per image\n",
    "\n",
    "            predicted = np.array([1 if err > threshold else 0 for err in error.detach().cpu().numpy()])\n",
    "            labels = labels.long()\n",
    "\n",
    "            loss = combined_loss(outputs, images)\n",
    "            total_val_loss += loss.item()\n",
    "            labels_cpu = labels.detach().cpu().numpy()\n",
    "            correct += (predicted == labels_cpu).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = 100. * correct / total\n",
    "\n",
    "    # Learning rate update\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Checkpoint\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'model_classification.pth')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.4f}%, Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.4f}%, Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
    "\n",
    "    # Store performance\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "6fe5031acf5e4d3cbf84da4a79d7dd91",
      "894cb53e5f7f4b2c897084cb01b3a966",
      "d3b7069fb39e44eabd4633141324bcaa",
      "697ba2f1d65346a99de0af453303a7f8",
      "9efc49f298e84728920827de8e56105d",
      "9ce57d92bdf944e387155dda3c8793f8",
      "a73e49a928e8418b9dd9fa55ae43e9de",
      "90d533458db6439ba86f3749605dcea0",
      "2c12543e4f7d4061a8a6abf50bd36093",
      "5d4109a21494406280be877d0b542b4a",
      "a0269e5c644b4da1a672a3f7883302a6"
     ]
    },
    "id": "6EQw1Uwr-lrx",
    "outputId": "24450337-6df9-4cc3-cb25-58991c474b69"
   },
   "outputs": [],
   "source": [
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('model_classification.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "file_paths = glob.glob(r'Dataset/cable/test/*/*.png')\n",
    "\n",
    "for img in tqdm(file_paths):\n",
    "    if img.split('/')[3] == 'good':\n",
    "      y_test.append(1)\n",
    "    else:\n",
    "      y_test.append(0)\n",
    "    img = cv2.imread(img)\n",
    "    img = img[..., ::-1]\n",
    "    x_test.append(img)\n",
    "\n",
    "x_test = np.stack(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "x_test = np.transpose(np.array(x_test), (0, 3, 1, 2))\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "test_dataset = MyDataset(x_test, y_test, val_transforms)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        reconstruction_errors = []\n",
    "        images = images.to(device)\n",
    "        images = (images) / 255.\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        error = torch.mean((outputs - images) ** 2, dim=[1, 2, 3])  # per image\n",
    "        #reconstruction_errors.extend(error.detach().cpu().numpy())\n",
    "        #threshold = np.percentile(reconstruction_errors, 95)  # e.g., top 5% as anomaly\n",
    "\n",
    "        predicted = np.array([1 if err > threshold else 0 for err in error.detach().cpu().numpy()])\n",
    "        labels = labels.long()\n",
    "\n",
    "\n",
    "        labels_cpu = labels.detach().cpu().numpy()\n",
    "        test_correct += (predicted == labels_cpu).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f'Test accuracy is {100. * test_correct / test_total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hG9-hyWBbtU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47c0HL9-DcE6"
   },
   "source": [
    "# Method4  Deep SVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnOGtIZvDjOk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "# Step 1: Define the encoder\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, encoded_dim=2048):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights='IMAGENET1K_V1')\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # remove fc layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.flatten(x)  # [batch, 2048]\n",
    "        return x\n",
    "class DeepSVDDTrainer:\n",
    "    def __init__(self, model, device, nu=0.1):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.nu = nu\n",
    "        self.c = None  # center of hypersphere\n",
    "\n",
    "    def initialize_center(self, dataloader):\n",
    "        \"\"\" Compute the mean of encoded features for center c \"\"\"\n",
    "        self.model.eval()\n",
    "        n_samples = 0\n",
    "        c = torch.zeros(self.model(torch.zeros(1, 3, 224, 224).to(self.device)).shape[1], device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, _ in dataloader:\n",
    "                x = x.to(self.device)\n",
    "                x = x / 255.\n",
    "                z = self.model(x)\n",
    "                n_samples += z.shape[0]\n",
    "                c += torch.sum(z, dim=0)\n",
    "\n",
    "        c /= n_samples\n",
    "\n",
    "        # If any dimension is close to 0, adjust it slightly to avoid trivial solutions\n",
    "        c[c.abs() < 1e-6] = 1e-6\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, dataloader, lr=1e-4, epochs=50):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x, _ in dataloader:\n",
    "                x = x.to(self.device)\n",
    "                x = x / 255.\n",
    "                z = self.model(x)\n",
    "                dist = torch.sum((z - self.c) ** 2, dim=1)  # squared L2 distance\n",
    "                loss = torch.mean(dist)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "def get_anomaly_scores(model, dataloader, center, device):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            x = x / 255.\n",
    "            z = model(x)\n",
    "            dist = torch.sum((z - center) ** 2, dim=1)\n",
    "            scores.extend(dist.cpu().numpy())\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKdI0tdyENbF",
    "outputId": "cd6e2e5f-4ad3-436a-a8de-f03205ab5d36"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "encoder = ResNetEncoder(encoded_dim=2048)\n",
    "svdd = DeepSVDDTrainer(encoder, device)\n",
    "\n",
    "svdd.initialize_center(train_loader)\n",
    "svdd.train(train_loader, lr=1e-4, epochs=50)\n",
    "\n",
    "train_scores = get_anomaly_scores(svdd.model, train_loader, svdd.c, device)\n",
    "test_scores = get_anomaly_scores(svdd.model, test_loader, svdd.c, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo-sjUpbEVxT"
   },
   "outputs": [],
   "source": [
    "threshold = np.percentile(train_scores, 95)\n",
    "\n",
    "# Predict\n",
    "predicted = np.array([1 if score > threshold else 0 for score in test_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3kHsIJlE0Nk",
    "outputId": "9764c1f9-2bb9-4e8b-920f-5e8d71f1a436"
   },
   "outputs": [],
   "source": [
    "accuracy = 100. * np.sum(predicted == y_test) / len(y_test)\n",
    "print(f'Test accuracy is {accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exAnY8WPFBpF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
