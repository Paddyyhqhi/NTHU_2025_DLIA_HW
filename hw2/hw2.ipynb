{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "###### Lab 2\n",
    "\n",
    "# National Tsing Hua University\n",
    "\n",
    "#### Spring 2025\n",
    "\n",
    "#### 11320IEEM 513600\n",
    "\n",
    "#### Deep Learning and Industrial Applications\n",
    "    \n",
    "## Lab 2: Predicting Heart Disease with Deep Learning\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963",
    "tags": []
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In the realm of healthcare, early detection and accurate prediction of diseases play a crucial role in patient care and management. Heart disease remains one of the leading causes of mortality worldwide, making the development of effective diagnostic tools essential. This lab leverages deep learning to predict the presence of heart disease in patients using a subset of 14 key attributes from the Cleveland Heart Disease Database. The objective is to explore and apply deep learning techniques to distinguish between the presence and absence of heart disease based on clinical parameters.\n",
    "\n",
    "Throughout this lab, you'll engage with the following key activities:\n",
    "- Use [Pandas](https://pandas.pydata.org) to process the CSV files.\n",
    "- Use [PyTorch](https://pytorch.org) to build an Artificial Neural Network (ANN) to fit the dataset.\n",
    "- Evaluate the performance of the trained model to understand its accuracy.\n",
    "\n",
    "### Attribute Information\n",
    "\n",
    "1. age: Age of the patient in years\n",
    "2. sex: (Male/Female)\n",
    "3. cp: Chest pain type (4 types: low, medium, high, and severe)\n",
    "4. trestbps: Resting blood pressure\n",
    "5. chol: Serum cholesterol in mg/dl\n",
    "6. fbs: Fasting blood sugar > 120 mg/dl\n",
    "7. restecg: Resting electrocardiographic results (values 0,1,2)\n",
    "8. thalach: Maximum heart rate achieved\n",
    "9. exang: Exercise induced angina\n",
    "10. oldpeak: Oldpeak = ST depression induced by exercise relative to rest\n",
    "11. slope: The slope of the peak exercise ST segment\n",
    "12. ca: Number of major vessels (0-3) colored by fluoroscopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversible defect\n",
    "14. target: target have disease or not (1=yes, 0=no)\n",
    "\n",
    "### References\n",
    "- [UCI Heart Disease Data](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data) for the dataset we use in this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "df = pd.read_csv('heart_dataset_train_all.csv')\n",
    "\n",
    "# one hot encoding\n",
    "sex_description = {\n",
    "    'Male': 0,\n",
    "    'Female': 1,\n",
    "}\n",
    "df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
    "\n",
    "# Mapping 'cp' (chest pain) descriptions to numbers\n",
    "pain_description = {\n",
    "    'low': 0,\n",
    "    'medium': 1,\n",
    "    'high': 2,\n",
    "    'severe': 3\n",
    "}\n",
    "df.loc[:, 'cp'] = df['cp'].map(pain_description)\n",
    "\n",
    "df = df.dropna()\n",
    "# split train, validation data\n",
    "np_data = df.values\n",
    "print(np_data.shape)\n",
    "split_point = int(np_data.shape[0]*0.7)\n",
    "\n",
    "np.random.shuffle(np_data)\n",
    "\n",
    "x_train = np_data[:split_point, :13]\n",
    "y_train = np_data[:split_point, 13]\n",
    "x_val = np_data[split_point:, :13]\n",
    "y_val = np_data[split_point:, 13]\n",
    "\n",
    "# trasform to Dataloader\n",
    "x_train = np.array(x_train, dtype=float)\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = np.array(y_train, dtype=int)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "x_val = np.array(x_val, dtype=float)\n",
    "x_val = torch.from_numpy(x_val).float()\n",
    "y_val = np.array(y_val, dtype=int)\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Get Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('heart_dataset_test.csv')\n",
    "test_data = test_data.values\n",
    "# Convert to PyTorch tensors\n",
    "x_test = torch.from_numpy(test_data[:, :13]).float()\n",
    "y_test = torch.from_numpy(test_data[:, 13]).long()\n",
    "\n",
    "# Create datasets\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(13, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "# print(model)\n",
    "\n",
    "\n",
    "\n",
    "# model = Model().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "\n",
    "def experiment(model,\n",
    "               train_loader,\n",
    "               val_loader,\n",
    "               test_loader,\n",
    "               lr_scheduler,\n",
    "               optimizer,\n",
    "               epochs=100)->Tuple[List, int]:\n",
    "    \"\"\"\n",
    "    Train & Evaluate the Model\n",
    "    Args:\n",
    "        train_loader, val_loader, test_loader -> Data\n",
    "        lr_scheduler, optimizer -> hyperparameters adjustment\n",
    "        \n",
    "    Returns:\n",
    "        history: [train_losses, val_losses, train_accuracies, val_accuracies, lrs]\n",
    "        test_accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    lrs = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = -1\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_predicted = outputs.argmax(-1)\n",
    "            train_correct += (train_predicted == labels).sum().item()\n",
    "            total_train_samples += labels.size(0)\n",
    "            \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        lrs.append(current_lr)\n",
    "        # Learning rate update\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = 100. * train_correct / total_train_samples\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(features)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                predicted = outputs.argmax(-1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = 100. * correct / total\n",
    "\n",
    "        # Checkpoint\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), 'model_classification.pth')\n",
    "\n",
    "        # Store performance\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "    #Test Section\n",
    "    # Load the trained weights\n",
    "    model.load_state_dict(torch.load('model_classification.pth'))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_correct = 0\n",
    "    total_test_loss = 0.0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            total_test_loss += loss.item()\n",
    "            \n",
    "            predicted = outputs.argmax(-1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            \n",
    "    test_loss = total_test_loss / len(test_loader)        \n",
    "    test_accuracy = 100. * test_correct / test_total\n",
    "    print(f'Test accuracy is {test_accuracy}%')\n",
    "    \n",
    "    history = [train_losses, val_losses, train_accuracies, val_accuracies, lrs]\n",
    "    \n",
    "    return history, test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "history, test_accuracy, test_loss = experiment(model,\n",
    "                                               train_loader,\n",
    "                                               val_loader,\n",
    "                                               test_loader,\n",
    "                                               lr_scheduler,\n",
    "                                                   optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Change Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr_param = [1e-2, 5*1e-3, 1e-4]\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
    "ax = ax.flatten()\n",
    "\n",
    "result = {\"learning rate\":[1e-2, 1e-2, 5*1e-3, 5*1e-3, 1e-4, 1e-4],\n",
    "          \"meaning\":[\"Accuracy\", \"Loss\", \"Accuracy\", \"Loss\", \"Accuracy\", \"Loss\"],\n",
    "          \"train\":[],\n",
    "          \"val\":[],\n",
    "          \"test\":[]}\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    model = Model().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_param[i])\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "    history, test_accuracy, test_loss = experiment(model,\n",
    "                                                   train_loader,\n",
    "                                                   val_loader,\n",
    "                                                   test_loader,\n",
    "                                                   lr_scheduler,\n",
    "                                                   optimizer)\n",
    "    \n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, lr = history\n",
    "    \n",
    "    result[\"train\"].append(train_accuracies[-1])\n",
    "    result[\"train\"].append(train_losses[-1])\n",
    "    result[\"val\"].append(val_accuracies[-1])\n",
    "    result[\"val\"].append(val_losses[-1])\n",
    "    result[\"test\"].append(test_accuracy)\n",
    "    result[\"test\"].append(test_loss)\n",
    "    \n",
    "    # Plotting training and validation accuracy\n",
    "    ax[i*3].plot(train_accuracies)\n",
    "    ax[i*3].plot(val_accuracies)\n",
    "    ax[i*3].set_title(f'Model Accuracy, Test Accuracy={test_accuracy:.4f}')\n",
    "    ax[i*3].set_xlabel('Epochs')\n",
    "    ax[i*3].set_ylabel('Accuracy')\n",
    "    ax[i*3].legend(['Train', 'Val'])\n",
    "\n",
    "    # Plotting training and validation loss\n",
    "    ax[i*3+1].plot(train_losses)\n",
    "    ax[i*3+1].plot(val_losses)\n",
    "    ax[i*3+1].set_title('Model Loss')\n",
    "    ax[i*3+1].set_xlabel('Epochs')\n",
    "    ax[i*3+1].set_ylabel('Loss')\n",
    "    ax[i*3+1].legend(['Train', 'Val'])\n",
    "    \n",
    "    # Plotting Change of Learning Rate\n",
    "    ax[i*3+2].plot(lr, label='learning rate')\n",
    "    ax[i*3+2].set_title('Learning Rate')\n",
    "    ax[i*3+2].set_xlabel('Epochs')\n",
    "    ax[i*3+2].set_ylabel('Loss')\n",
    "    ax[i*3+2].legend()\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.2, hspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)\n",
    "df = df.groupby([\"learning rate\", \"meaning\"]).mean().sort_values([\"learning rate\"], ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Change Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = [(0.9, 0.999), (0.5, 0.6), (0.1, 0.2)]\n",
    "fig, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
    "ax = ax.flatten()\n",
    "\n",
    "result = {\"beta\":[(0.9, 0.95), (0.9, 0.95), (0.5, 0.6), (0.5, 0.6), (0.1, 0.2), (0.1, 0.2)],\n",
    "          \"meaning\":[\"Accuracy\", \"Loss\", \"Accuracy\", \"Loss\", \"Accuracy\", \"Loss\"],\n",
    "          \"train\":[],\n",
    "          \"val\":[],\n",
    "          \"test\":[]}\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    model = Model().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=betas[i])\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "    history, test_accuracy, test_loss = experiment(model,\n",
    "                                        train_loader,\n",
    "                                        val_loader,\n",
    "                                        test_loader,\n",
    "                                        lr_scheduler,\n",
    "                                        optimizer)\n",
    "    \n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, lr = history\n",
    "    \n",
    "    result[\"train\"].append(train_accuracies[-1])\n",
    "    result[\"train\"].append(train_losses[-1])\n",
    "    result[\"val\"].append(val_accuracies[-1])\n",
    "    result[\"val\"].append(val_losses[-1])\n",
    "    result[\"test\"].append(test_accuracy)\n",
    "    result[\"test\"].append(test_loss)\n",
    "\n",
    "\n",
    "    # Plotting training and validation accuracy\n",
    "    ax[i*3].plot(train_accuracies)\n",
    "    ax[i*3].plot(val_accuracies)\n",
    "    ax[i*3].set_title(f'Model Accuracy, Test Accuracy={test_accuracy:.4f}')\n",
    "    ax[i*3].set_xlabel('Epochs')\n",
    "    ax[i*3].set_ylabel('Accuracy')\n",
    "    ax[i*3].legend(['Train', 'Val'])\n",
    "\n",
    "    # Plotting training and validation loss\n",
    "    ax[i*3+1].plot(train_losses)\n",
    "    ax[i*3+1].plot(val_losses)\n",
    "    ax[i*3+1].set_title('Model Loss')\n",
    "    ax[i*3+1].set_xlabel('Epochs')\n",
    "    ax[i*3+1].set_ylabel('Loss')\n",
    "    ax[i*3+1].legend(['Train', 'Val'])\n",
    "    \n",
    "    # Plotting Change of Learning Rate\n",
    "    ax[i*3+2].plot(lr, label='learning rate')\n",
    "    ax[i*3+2].set_title('Learning Rate')\n",
    "    ax[i*3+2].set_xlabel('Epochs')\n",
    "    ax[i*3+2].set_ylabel('Loss')\n",
    "    ax[i*3+2].legend()\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.2, hspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)\n",
    "df = df.groupby([\"beta\", \"meaning\"]).mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Differences between Training Data and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('heart_dataset_test.csv')\n",
    "\n",
    "df = pd.read_csv('heart_dataset_train_all.csv')\n",
    "\n",
    "# one hot encoding\n",
    "sex_description = {\n",
    "    'Male': 0,\n",
    "    'Female': 1,\n",
    "}\n",
    "df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
    "\n",
    "# Mapping 'cp' (chest pain) descriptions to numbers\n",
    "pain_description = {\n",
    "    'low': 0,\n",
    "    'medium': 1,\n",
    "    'high': 2,\n",
    "    'severe': 3\n",
    "}\n",
    "df.loc[:, 'cp'] = df['cp'].map(pain_description)\n",
    "\n",
    "df_train = df.dropna()\n",
    "features = df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "for col in features:\n",
    "    stat, p_value = ks_2samp(df_train[col], df_test[col])\n",
    "    print(f\"{col}: KS statistic = {stat:.4f}, p = {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "for col in [\"sex\", \"cp\"]:\n",
    "    train_counts = df_train[col].value_counts()\n",
    "    test_counts = df_test[col].value_counts()\n",
    "    all_categories = set(train_counts.index).union(set(test_counts.index))\n",
    "\n",
    "    train_freq = [train_counts.get(cat, 0) for cat in all_categories]\n",
    "    test_freq = [test_counts.get(cat, 0) for cat in all_categories]\n",
    "\n",
    "    stat, p_value, _, _ = chi2_contingency([train_freq, test_freq])\n",
    "    print(f\"{col}: Chi2 p = {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Feature Selection & Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "ax = sns.heatmap(df_train.corr(), cmap=\"Purples\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\n",
    "\n",
    "X = df_train.drop(columns=\"target\")\n",
    "y = df_train[\"target\"]\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=10)  # Or use mutual_info_classif\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected features:\", selected_features.tolist())\n",
    "selected_features = selected_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(selected_features)\n",
    "selected_features.append(\"target\")\n",
    "df = df_train[selected_features]\n",
    "df = df.dropna()\n",
    "# split train, validation data\n",
    "np_data = df.values\n",
    "print(np_data.shape)\n",
    "split_point = int(np_data.shape[0]*0.7)\n",
    "\n",
    "np.random.shuffle(np_data)\n",
    "\n",
    "x_train = np_data[:split_point, :n]\n",
    "y_train = np_data[:split_point, n]\n",
    "x_val = np_data[split_point:, :n]\n",
    "y_val = np_data[split_point:, n]\n",
    "\n",
    "# trasform to Dataloader\n",
    "x_train = np.array(x_train, dtype=float)\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = np.array(y_train, dtype=int)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "x_val = np.array(x_val, dtype=float)\n",
    "x_val = torch.from_numpy(x_val).float()\n",
    "y_val = np.array(y_val, dtype=int)\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')\n",
    "\n",
    "test_data = df_test[selected_features]\n",
    "test_data = test_data.values\n",
    "# Convert to PyTorch tensors\n",
    "x_test = torch.from_numpy(test_data[:, :n]).float()\n",
    "y_test = torch.from_numpy(test_data[:, n]).long()\n",
    "\n",
    "# Create datasets\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_channel, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "model = Model(n).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "histroy, test_accuracy = experiment(model,\n",
    "                                    train_loader,\n",
    "                                    val_loader,\n",
    "                                    test_loader,\n",
    "                                    lr_scheduler,\n",
    "                                    optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, d_model, nhead, num_layers, num_classes=1):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        self.feature_embedding = nn.Linear(1, d_model)  # or use Embedding for categorical\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_features, d_model))\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features * d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, num_features]\n",
    "        x = x.unsqueeze(-1)  # [batch, num_features, 1]\n",
    "        x = self.feature_embedding(x)  # [batch, num_features, d_model]\n",
    "        x = x + self.pos_embedding  # Add positional embedding\n",
    "        x = self.transformer(x)  # [batch, num_features, d_model]\n",
    "        out = self.classifier(x)  # [batch, 1]\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model = TabularTransformer(13, 256, 4, 1, num_classes=2)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0)\n",
    "history, test_accuracy, test_loss = experiment(model,\n",
    "                                    train_loader,\n",
    "                                    val_loader,\n",
    "                                    test_loader,\n",
    "                                    lr_scheduler,\n",
    "                                    optimizer,\n",
    "                                   epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, _ = history\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "ax[0].plot(train_accuracies)\n",
    "ax[0].plot(val_accuracies)\n",
    "ax[0].set_title('Model Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['Train', 'Val'])\n",
    "\n",
    "# Plotting training and validation loss\n",
    "ax[1].plot(train_losses)\n",
    "ax[1].plot(val_losses)\n",
    "ax[1].set_title('Model Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['Train', 'Val'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
